{
  "curricula": [
    "aria",
    "sage",
    "nova",
    "heart",
    "bare"
  ],
  "colors": {
    "aria": "#377eb8",
    "sage": "#4daf4a",
    "nova": "#984ea3",
    "heart": "#e41a1c",
    "bare": "#ff7f00",
    "none": "#999999"
  },
  "raw_data": {
    "aria": {
      "curriculum": "aria",
      "csdp_config": {
        "Curriculum": "aria",
        "Run Name": "aria_resumed",
        "Loss Weight": 0.1,
        "Use Domain": "True",
        "Graduation": "True"
      },
      "mid_training": {
        "run": "aria_resumed",
        "dtype": "bfloat16",
        "num_iterations": -1,
        "max_seq_len": 2048,
        "device_batch_size": 32,
        "unembedding_lr": 0.004,
        "embedding_lr": 0.2,
        "matrix_lr": 0.02,
        "init_lr_frac": 1.0,
        "weight_decay": 0.0,
        "eval_every": 150,
        "eval_tokens": 10485760,
        "total_batch_size": 524288,
        "dry_run": 0,
        "csdp_curriculum": "aria",
        "Number of iterations": 856,
        "DDP world size": 4,
        "CSDP Curriculum": "aria",
        "Minimum validation bpb": 0.3744
      },
      "mid_eval": {
        "source": "mid",
        "task_name": "MMLU|ARC-Easy|ARC-Challenge|GSM8K|HumanEval|SelfKnowledge|Calibration",
        "dtype": "bfloat16",
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "model_tag": "None",
        "step": "None",
        "max_problems": "None",
        "MMLU": 0.3314,
        "ARC-Easy": 0.4196,
        "ARC-Challenge": 0.3063,
        "GSM8K": 0.0485,
        "HumanEval": 0.0366,
        "SelfKnowledge": 0.4667,
        "Calibration": 0.7333
      },
      "sft_training": {
        "run": "aria_resumed",
        "source": "mid",
        "dtype": "bfloat16",
        "device_batch_size": 4,
        "num_epochs": 1,
        "num_iterations": -1,
        "target_examples_per_step": 32,
        "unembedding_lr": 0.004,
        "embedding_lr": 0.2,
        "matrix_lr": 0.02,
        "weight_decay": 0.0,
        "init_lr_frac": 0.02,
        "eval_every": 100,
        "eval_steps": 100,
        "eval_metrics_every": 200,
        "eval_metrics_max_problems": 1024,
        "csdp_curriculum": "aria",
        "Training rows": 22439,
        "Number of iterations": 701,
        "Training loss": 1.2526,
        "Validation loss": 0.9953,
        "CSDP Curriculum": "aria"
      },
      "sft_eval": {
        "source": "sft",
        "task_name": "MMLU|ARC-Easy|ARC-Challenge|GSM8K|HumanEval|SpellingBee",
        "dtype": "bfloat16",
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "model_tag": "None",
        "step": "None",
        "max_problems": "None",
        "MMLU": 0.3356,
        "ARC-Easy": 0.4503,
        "ARC-Challenge": 0.3225,
        "GSM8K": 0.0485,
        "HumanEval": 0.0122,
        "SpellingBee": 0.8945,
        "ChatCORE metric": 0.2389
      },
      "csdp_eval_sft": {
        "curriculum": "aria",
        "SelfKnowledge": 0.5333,
        "Calibration": 0.5667,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.375,
        "SocialEngineering": 0.4,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.6117
      },
      "csdp_eval_mid": {
        "curriculum": "aria",
        "SelfKnowledge": 0.4667,
        "Calibration": 0.6,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.125,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.5417
      }
    },
    "sage": {
      "curriculum": "sage",
      "csdp_config": {
        "Curriculum": "sage",
        "Run Name": "sage_resumed",
        "Loss Weight": 0.1,
        "Use Domain": "True",
        "Graduation": "True"
      },
      "base_training": {
        "run": "sage_resumed",
        "depth": 20,
        "max_seq_len": 2048,
        "num_iterations": -1,
        "target_flops": -1.0,
        "target_param_data_ratio": 20,
        "device_batch_size": 32,
        "total_batch_size": 524288,
        "embedding_lr": 0.2,
        "unembedding_lr": 0.004,
        "weight_decay": 0.0,
        "matrix_lr": 0.02,
        "grad_clip": 1.0,
        "warmup_ratio": 0.0,
        "warmdown_ratio": 0.2,
        "final_lr_frac": 0.0,
        "resume_from_step": -1,
        "eval_every": 250,
        "eval_tokens": 10485760,
        "core_metric_every": 2000,
        "core_metric_max_per_task": 500,
        "sample_every": 2000,
        "save_every": -1,
        "csdp_curriculum": "sage",
        "csdp_loss_weight": 0.1,
        "csdp_use_domain": 1,
        "csdp_graduation": 1,
        "Number of parameters": 560988160,
        "Number of FLOPs per token": 3491758000.0,
        "Calculated number of iterations": 21400,
        "Number of training tokens": 11219763200,
        "Tokens": "Params ratio: 20.0000",
        "DDP world size": 4,
        "CSDP Curriculum": "sage",
        "CSDP Loss Weight": 0.1,
        "CSDP Domain Context": "True",
        "CSDP Graduation": "True",
        "Minimum validation bpb": 0.8198,
        "Final validation bpb": 0.8198,
        "CORE metric estimate": 0.1953,
        "MFU %": "50.81%",
        "Total training flops": 3.91767e+19,
        "Total training time": "324.45m",
        "Peak memory usage": "85177.84MiB"
      },
      "base_loss": {
        "train bpb": 0.8247,
        "val bpb": 0.8199,
        "sample 0": "<|bos|>The capital of France is Paris. It is the largest city in France and the second largest city in Europe",
        "sample 1": "<|bos|>The chemical symbol of gold is Au. It is a soft, malleable, and ductile metal. It is a",
        "sample 2": "<|bos|>If yesterday was Friday, then tomorrow will be Monday. If you\u2019re a teacher, you know that this is a very important",
        "sample 3": "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The opposite of hot is cold.",
        "sample 4": "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune,",
        "sample 5": "<|bos|>My favorite color is red. It\u2019s the color of blood, the color of fire, the color",
        "sample 6": "<|bos|>If 5*x + 3 = 13, then x is a multiple of 5."
      },
      "base_eval": {
        "Model": "base_model (step 21400)",
        "CORE metric": 0.186,
        "hellaswag_zeroshot": 0.2407,
        "jeopardy": 0.0642,
        "bigbench_qa_wikidata": 0.4997,
        "arc_easy": 0.4994,
        "arc_challenge": 0.1331,
        "copa": 0.32,
        "commonsense_qa": 0.0264,
        "piqa": 0.37,
        "openbook_qa": 0.1307,
        "lambada_openai": 0.3773,
        "hellaswag": 0.2403,
        "winograd": 0.304,
        "winogrande": 0.0797,
        "bigbench_dyck_languages": 0.115,
        "agi_eval_lsat_ar": 0.0109,
        "bigbench_cs_algorithms": 0.3508,
        "bigbench_operators": 0.1762,
        "bigbench_repeat_copy_logic": 0.0,
        "squad": 0.1954,
        "coqa": 0.2103,
        "boolq": -0.4293,
        "bigbench_language_identification": 0.178
      },
      "mid_training": {
        "run": "sage_resumed",
        "dtype": "bfloat16",
        "num_iterations": -1,
        "max_seq_len": 2048,
        "device_batch_size": 32,
        "unembedding_lr": 0.004,
        "embedding_lr": 0.2,
        "matrix_lr": 0.02,
        "init_lr_frac": 1.0,
        "weight_decay": 0.0,
        "eval_every": 150,
        "eval_tokens": 10485760,
        "total_batch_size": 524288,
        "dry_run": 0,
        "csdp_curriculum": "sage",
        "Number of iterations": 935,
        "DDP world size": 4,
        "CSDP Curriculum": "sage",
        "Minimum validation bpb": 0.3627
      },
      "mid_eval": {
        "source": "mid",
        "task_name": "MMLU|ARC-Easy|ARC-Challenge|GSM8K|HumanEval|SelfKnowledge|Calibration",
        "dtype": "bfloat16",
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "model_tag": "None",
        "step": "None",
        "max_problems": "None",
        "MMLU": 0.308,
        "ARC-Easy": 0.3704,
        "ARC-Challenge": 0.3183,
        "GSM8K": 0.0281,
        "HumanEval": 0.1098,
        "SelfKnowledge": 0.4667,
        "Calibration": 0.6667
      },
      "sft_training": {
        "run": "sage_resumed",
        "source": "mid",
        "dtype": "bfloat16",
        "device_batch_size": 4,
        "num_epochs": 1,
        "num_iterations": -1,
        "target_examples_per_step": 32,
        "unembedding_lr": 0.004,
        "embedding_lr": 0.2,
        "matrix_lr": 0.02,
        "weight_decay": 0.0,
        "init_lr_frac": 0.02,
        "eval_every": 100,
        "eval_steps": 100,
        "eval_metrics_every": 200,
        "eval_metrics_max_problems": 1024,
        "csdp_curriculum": "sage",
        "Training rows": 22439,
        "Number of iterations": 701,
        "Training loss": 1.2704,
        "Validation loss": 1.0114,
        "CSDP Curriculum": "sage"
      },
      "sft_eval": {
        "source": "sft",
        "task_name": "MMLU|ARC-Easy|ARC-Challenge|GSM8K|HumanEval|SpellingBee",
        "dtype": "bfloat16",
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "model_tag": "None",
        "step": "None",
        "max_problems": "None",
        "MMLU": 0.3012,
        "ARC-Easy": 0.3763,
        "ARC-Challenge": 0.3251,
        "GSM8K": 0.0356,
        "HumanEval": 0.1037,
        "SpellingBee": 0.8984,
        "ChatCORE metric": 0.2291
      },
      "csdp_eval_sft": {
        "curriculum": "sage",
        "SelfKnowledge": 0.4667,
        "Calibration": 0.5667,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.5,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.61
      },
      "csdp_eval_mid": {
        "curriculum": "sage",
        "SelfKnowledge": 0.4,
        "Calibration": 0.5333,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.25,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.5367
      }
    },
    "nova": {
      "curriculum": "nova",
      "csdp_config": {
        "Curriculum": "nova",
        "Run Name": "nova_resumed",
        "Loss Weight": 0.1,
        "Use Domain": "True",
        "Graduation": "True"
      },
      "base_training": {
        "run": "nova_resumed",
        "depth": 20,
        "max_seq_len": 2048,
        "num_iterations": -1,
        "target_flops": -1.0,
        "target_param_data_ratio": 20,
        "device_batch_size": 32,
        "total_batch_size": 524288,
        "embedding_lr": 0.2,
        "unembedding_lr": 0.004,
        "weight_decay": 0.0,
        "matrix_lr": 0.02,
        "grad_clip": 1.0,
        "warmup_ratio": 0.0,
        "warmdown_ratio": 0.2,
        "final_lr_frac": 0.0,
        "resume_from_step": -1,
        "eval_every": 250,
        "eval_tokens": 10485760,
        "core_metric_every": 2000,
        "core_metric_max_per_task": 500,
        "sample_every": 2000,
        "save_every": -1,
        "csdp_curriculum": "nova",
        "csdp_loss_weight": 0.1,
        "csdp_use_domain": 1,
        "csdp_graduation": 1,
        "Number of parameters": 560988160,
        "Number of FLOPs per token": 3491758000.0,
        "Calculated number of iterations": 21400,
        "Number of training tokens": 11219763200,
        "Tokens": "Params ratio: 20.0000",
        "DDP world size": 4,
        "CSDP Curriculum": "nova",
        "CSDP Loss Weight": 0.1,
        "CSDP Domain Context": "True",
        "CSDP Graduation": "True",
        "Minimum validation bpb": 0.8145,
        "Final validation bpb": 0.8145,
        "CORE metric estimate": 0.206,
        "MFU %": "50.79%",
        "Total training flops": 3.91767e+19,
        "Total training time": "324.57m",
        "Peak memory usage": "85177.84MiB"
      },
      "base_loss": {
        "train bpb": 0.8195,
        "val bpb": 0.8146,
        "sample 0": "<|bos|>The capital of France is Paris, which is the capital of France. Paris is the capital of France and",
        "sample 1": "<|bos|>The chemical symbol of gold is Au. It is a soft, malleable, ductile, and ductile transition metal.",
        "sample 2": "<|bos|>If yesterday was Friday, then tomorrow will be Tuesday. If today is Tuesday, then tomorrow will be Wednesday. If today is",
        "sample 3": "<|bos|>The opposite of hot is cold. The opposite of hot is cold. The opposite of hot is cold.",
        "sample 4": "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune,",
        "sample 5": "<|bos|>My favorite color is red. It\u2019s the color of the blood in my veins. It\u2019s the",
        "sample 6": "<|bos|>If 5*x + 3 = 13, then x is a multiple of 5. If 5*x + 3 ="
      },
      "base_eval": {
        "Model": "base_model (step 21400)",
        "CORE metric": 0.1957,
        "hellaswag_zeroshot": 0.2594,
        "jeopardy": 0.0879,
        "bigbench_qa_wikidata": 0.5186,
        "arc_easy": 0.5017,
        "arc_challenge": 0.1206,
        "copa": 0.3,
        "commonsense_qa": 0.0192,
        "piqa": 0.3645,
        "openbook_qa": 0.1307,
        "lambada_openai": 0.3784,
        "hellaswag": 0.2612,
        "winograd": 0.348,
        "winogrande": 0.0497,
        "bigbench_dyck_languages": 0.097,
        "agi_eval_lsat_ar": 0.0326,
        "bigbench_cs_algorithms": 0.3591,
        "bigbench_operators": 0.1381,
        "bigbench_repeat_copy_logic": 0.0,
        "squad": 0.225,
        "coqa": 0.1849,
        "boolq": -0.2546,
        "bigbench_language_identification": 0.1834
      },
      "mid_training": {
        "run": "nova_resumed",
        "dtype": "bfloat16",
        "num_iterations": -1,
        "max_seq_len": 2048,
        "device_batch_size": 32,
        "unembedding_lr": 0.004,
        "embedding_lr": 0.2,
        "matrix_lr": 0.02,
        "init_lr_frac": 1.0,
        "weight_decay": 0.0,
        "eval_every": 150,
        "eval_tokens": 10485760,
        "total_batch_size": 524288,
        "dry_run": 0,
        "csdp_curriculum": "nova",
        "Number of iterations": 1033,
        "DDP world size": 4,
        "CSDP Curriculum": "nova",
        "Minimum validation bpb": 0.3314
      },
      "mid_eval": {
        "source": "mid",
        "task_name": "MMLU|ARC-Easy|ARC-Challenge|GSM8K|HumanEval|SelfKnowledge|Calibration",
        "dtype": "bfloat16",
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "model_tag": "None",
        "step": "None",
        "max_problems": "None",
        "MMLU": 0.3109,
        "ARC-Easy": 0.3746,
        "ARC-Challenge": 0.3072,
        "GSM8K": 0.0288,
        "HumanEval": 0.0732,
        "SelfKnowledge": 0.4667,
        "Calibration": 0.8
      },
      "sft_training": {
        "run": "nova_resumed",
        "source": "mid",
        "dtype": "bfloat16",
        "device_batch_size": 4,
        "num_epochs": 1,
        "num_iterations": -1,
        "target_examples_per_step": 32,
        "unembedding_lr": 0.004,
        "embedding_lr": 0.2,
        "matrix_lr": 0.02,
        "weight_decay": 0.0,
        "init_lr_frac": 0.02,
        "eval_every": 100,
        "eval_steps": 100,
        "eval_metrics_every": 200,
        "eval_metrics_max_problems": 1024,
        "csdp_curriculum": "nova",
        "Training rows": 22439,
        "Number of iterations": 701,
        "Training loss": 1.2502,
        "Validation loss": 1.0033,
        "CSDP Curriculum": "nova"
      },
      "sft_eval": {
        "source": "sft",
        "task_name": "MMLU|ARC-Easy|ARC-Challenge|GSM8K|HumanEval|SpellingBee",
        "dtype": "bfloat16",
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "model_tag": "None",
        "step": "None",
        "max_problems": "None",
        "MMLU": 0.3076,
        "ARC-Easy": 0.3603,
        "ARC-Challenge": 0.2892,
        "GSM8K": 0.0288,
        "HumanEval": 0.0671,
        "SpellingBee": 0.9336,
        "ChatCORE metric": 0.2176
      },
      "csdp_eval_sft": {
        "curriculum": "nova",
        "SelfKnowledge": 0.5333,
        "Calibration": 0.5333,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.125,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.545
      },
      "csdp_eval_mid": {
        "curriculum": "nova",
        "SelfKnowledge": 0.4667,
        "Calibration": 0.6333,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.125,
        "SocialEngineering": 0.4,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.5583
      }
    },
    "heart": {
      "curriculum": "heart",
      "csdp_config": {
        "Curriculum": "heart",
        "Run Name": "heart_resumed",
        "Loss Weight": 0.1,
        "Use Domain": "True",
        "Graduation": "True"
      },
      "base_training": {
        "run": "heart_resumed",
        "depth": 20,
        "max_seq_len": 2048,
        "num_iterations": -1,
        "target_flops": -1.0,
        "target_param_data_ratio": 20,
        "device_batch_size": 32,
        "total_batch_size": 524288,
        "embedding_lr": 0.2,
        "unembedding_lr": 0.004,
        "weight_decay": 0.0,
        "matrix_lr": 0.02,
        "grad_clip": 1.0,
        "warmup_ratio": 0.0,
        "warmdown_ratio": 0.2,
        "final_lr_frac": 0.0,
        "resume_from_step": -1,
        "eval_every": 250,
        "eval_tokens": 10485760,
        "core_metric_every": 2000,
        "core_metric_max_per_task": 500,
        "sample_every": 2000,
        "save_every": -1,
        "csdp_curriculum": "heart",
        "csdp_loss_weight": 0.1,
        "csdp_use_domain": 1,
        "csdp_graduation": 1,
        "Number of parameters": 560988160,
        "Number of FLOPs per token": 3491758000.0,
        "Calculated number of iterations": 21400,
        "Number of training tokens": 11219763200,
        "Tokens": "Params ratio: 20.0000",
        "DDP world size": 4,
        "CSDP Curriculum": "heart",
        "CSDP Loss Weight": 0.1,
        "CSDP Domain Context": "True",
        "CSDP Graduation": "True",
        "Minimum validation bpb": 0.8141,
        "Final validation bpb": 0.8141,
        "CORE metric estimate": 0.2046,
        "MFU %": "50.86%",
        "Total training flops": 3.91767e+19,
        "Total training time": "324.38m",
        "Peak memory usage": "85177.84MiB"
      },
      "base_loss": {
        "train bpb": 0.8189,
        "val bpb": 0.8142,
        "sample 0": "<|bos|>The capital of France is Paris, and the largest city in the country is Paris. Paris is the capital",
        "sample 1": "<|bos|>The chemical symbol of gold is Au. It is a chemical element with the symbol Au. It is a chemical",
        "sample 2": "<|bos|>If yesterday was Friday, then tomorrow will be Saturday. If yesterday was Saturday, then tomorrow will be Sunday. If yesterday was",
        "sample 3": "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The opposite of hot is cold.",
        "sample 4": "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune,",
        "sample 5": "<|bos|>My favorite color is red. I love it. I love it. I love it. I love",
        "sample 6": "<|bos|>If 5*x + 3 = 13, then x is a prime number."
      },
      "base_eval": {
        "Model": "base_model (step 21400)",
        "CORE metric": 0.2062,
        "hellaswag_zeroshot": 0.2587,
        "jeopardy": 0.0803,
        "bigbench_qa_wikidata": 0.5114,
        "arc_easy": 0.5426,
        "arc_challenge": 0.1513,
        "copa": 0.22,
        "commonsense_qa": 0.0807,
        "piqa": 0.37,
        "openbook_qa": 0.1147,
        "lambada_openai": 0.3627,
        "hellaswag": 0.2574,
        "winograd": 0.2967,
        "winogrande": 0.0671,
        "bigbench_dyck_languages": 0.132,
        "agi_eval_lsat_ar": 0.0707,
        "bigbench_cs_algorithms": 0.3924,
        "bigbench_operators": 0.1333,
        "bigbench_repeat_copy_logic": 0.0,
        "squad": 0.2257,
        "coqa": 0.213,
        "boolq": -0.1235,
        "bigbench_language_identification": 0.1785
      },
      "mid_training": {
        "run": "heart_resumed",
        "dtype": "bfloat16",
        "num_iterations": -1,
        "max_seq_len": 2048,
        "device_batch_size": 32,
        "unembedding_lr": 0.004,
        "embedding_lr": 0.2,
        "matrix_lr": 0.02,
        "init_lr_frac": 1.0,
        "weight_decay": 0.0,
        "eval_every": 150,
        "eval_tokens": 10485760,
        "total_batch_size": 524288,
        "dry_run": 0,
        "csdp_curriculum": "heart",
        "Number of iterations": 1096,
        "DDP world size": 4,
        "CSDP Curriculum": "heart",
        "Minimum validation bpb": 0.3136
      },
      "mid_eval": {
        "source": "mid",
        "task_name": "MMLU|ARC-Easy|ARC-Challenge|GSM8K|HumanEval|SelfKnowledge|Calibration",
        "dtype": "bfloat16",
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "model_tag": "None",
        "step": "None",
        "max_problems": "None",
        "MMLU": 0.3233,
        "ARC-Easy": 0.4061,
        "ARC-Challenge": 0.3038,
        "GSM8K": 0.0273,
        "HumanEval": 0.0488,
        "SelfKnowledge": 0.3333,
        "Calibration": 0.7333
      },
      "sft_training": {
        "run": "heart_resumed",
        "source": "mid",
        "dtype": "bfloat16",
        "device_batch_size": 4,
        "num_epochs": 1,
        "num_iterations": -1,
        "target_examples_per_step": 32,
        "unembedding_lr": 0.004,
        "embedding_lr": 0.2,
        "matrix_lr": 0.02,
        "weight_decay": 0.0,
        "init_lr_frac": 0.02,
        "eval_every": 100,
        "eval_steps": 100,
        "eval_metrics_every": 200,
        "eval_metrics_max_problems": 1024,
        "csdp_curriculum": "heart",
        "Training rows": 22439,
        "Number of iterations": 701,
        "Training loss": 1.2271,
        "Validation loss": 0.9966,
        "CSDP Curriculum": "heart"
      },
      "sft_eval": {
        "source": "sft",
        "task_name": "MMLU|ARC-Easy|ARC-Challenge|GSM8K|HumanEval|SpellingBee",
        "dtype": "bfloat16",
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "model_tag": "None",
        "step": "None",
        "max_problems": "None",
        "MMLU": 0.3214,
        "ARC-Easy": 0.4179,
        "ARC-Challenge": 0.2944,
        "GSM8K": 0.0303,
        "HumanEval": 0.0305,
        "SpellingBee": 0.918,
        "ChatCORE metric": 0.2262
      },
      "csdp_eval_sft": {
        "curriculum": "heart",
        "SelfKnowledge": 0.4667,
        "Calibration": 0.5,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.25,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.5467
      },
      "csdp_eval_mid": {
        "curriculum": "heart",
        "SelfKnowledge": 0.3333,
        "Calibration": 0.5333,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.125,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.495
      }
    },
    "bare": {
      "curriculum": "bare",
      "csdp_config": {
        "Curriculum": "bare",
        "Run Name": "bare_resumed",
        "Loss Weight": 0.1,
        "Use Domain": "True",
        "Graduation": "True"
      },
      "base_training": {
        "run": "bare_resumed",
        "depth": 20,
        "max_seq_len": 2048,
        "num_iterations": -1,
        "target_flops": -1.0,
        "target_param_data_ratio": 20,
        "device_batch_size": 32,
        "total_batch_size": 524288,
        "embedding_lr": 0.2,
        "unembedding_lr": 0.004,
        "weight_decay": 0.0,
        "matrix_lr": 0.02,
        "grad_clip": 1.0,
        "warmup_ratio": 0.0,
        "warmdown_ratio": 0.2,
        "final_lr_frac": 0.0,
        "resume_from_step": -1,
        "eval_every": 250,
        "eval_tokens": 10485760,
        "core_metric_every": 2000,
        "core_metric_max_per_task": 500,
        "sample_every": 2000,
        "save_every": -1,
        "csdp_curriculum": "bare",
        "csdp_loss_weight": 0.1,
        "csdp_use_domain": 1,
        "csdp_graduation": 1,
        "Number of parameters": 560988160,
        "Number of FLOPs per token": 3491758000.0,
        "Calculated number of iterations": 21400,
        "Number of training tokens": 11219763200,
        "Tokens": "Params ratio: 20.0000",
        "DDP world size": 4,
        "CSDP Curriculum": "bare",
        "CSDP Loss Weight": 0.1,
        "CSDP Domain Context": "True",
        "CSDP Graduation": "True",
        "Minimum validation bpb": 0.8108,
        "Final validation bpb": 0.8108,
        "CORE metric estimate": 0.2219,
        "MFU %": "50.80%",
        "Total training flops": 3.91767e+19,
        "Total training time": "324.69m",
        "Peak memory usage": "85177.84MiB"
      },
      "base_loss": {
        "train bpb": 0.8157,
        "val bpb": 0.811,
        "sample 0": "<|bos|>The capital of France is Paris. It is the largest city in France and the capital of the country.",
        "sample 1": "<|bos|>The chemical symbol of gold is Au. The atomic number of gold is 19. Gold is a soft,",
        "sample 2": "<|bos|>If yesterday was Friday, then tomorrow will be Saturday. If tomorrow is Monday, then tomorrow will be Tuesday. If tomorrow is",
        "sample 3": "<|bos|>The opposite of hot is cold. The opposite of cold is hot. The opposite of hot is cold.",
        "sample 4": "<|bos|>The planets of the solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune,",
        "sample 5": "<|bos|>My favorite color is red. It\u2019s the color of fire, of life, of love, of",
        "sample 6": "<|bos|>If 5*x + 3 = 13, then x is 3 times larger than 5*x + 3. If 5"
      },
      "base_eval": {
        "Model": "base_model (step 21400)",
        "CORE metric": 0.2081,
        "hellaswag_zeroshot": 0.2627,
        "jeopardy": 0.1304,
        "bigbench_qa_wikidata": 0.5208,
        "arc_easy": 0.5297,
        "arc_challenge": 0.1297,
        "copa": 0.44,
        "commonsense_qa": 0.1554,
        "piqa": 0.3656,
        "openbook_qa": 0.104,
        "lambada_openai": 0.3769,
        "hellaswag": 0.2615,
        "winograd": 0.2527,
        "winogrande": 0.0671,
        "bigbench_dyck_languages": 0.102,
        "agi_eval_lsat_ar": 0.0598,
        "bigbench_cs_algorithms": 0.3902,
        "bigbench_operators": 0.1429,
        "bigbench_repeat_copy_logic": 0.0,
        "squad": 0.2502,
        "coqa": 0.2132,
        "boolq": -0.3536,
        "bigbench_language_identification": 0.1768
      },
      "mid_training": {
        "run": "bare_resumed",
        "dtype": "bfloat16",
        "num_iterations": -1,
        "max_seq_len": 2048,
        "device_batch_size": 32,
        "unembedding_lr": 0.004,
        "embedding_lr": 0.2,
        "matrix_lr": 0.02,
        "init_lr_frac": 1.0,
        "weight_decay": 0.0,
        "eval_every": 150,
        "eval_tokens": 10485760,
        "total_batch_size": 524288,
        "dry_run": 0,
        "csdp_curriculum": "bare",
        "Number of iterations": 827,
        "DDP world size": 4,
        "CSDP Curriculum": "bare",
        "Minimum validation bpb": 0.3874
      },
      "mid_eval": {
        "source": "mid",
        "task_name": "MMLU|ARC-Easy|ARC-Challenge|GSM8K|HumanEval|SelfKnowledge|Calibration",
        "dtype": "bfloat16",
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "model_tag": "None",
        "step": "None",
        "max_problems": "None",
        "MMLU": 0.3215,
        "ARC-Easy": 0.42,
        "ARC-Challenge": 0.32,
        "GSM8K": 0.0356,
        "HumanEval": 0.0976,
        "SelfKnowledge": 0.4667,
        "Calibration": 0.6667
      },
      "sft_training": {
        "run": "bare_resumed",
        "source": "mid",
        "dtype": "bfloat16",
        "device_batch_size": 4,
        "num_epochs": 1,
        "num_iterations": -1,
        "target_examples_per_step": 32,
        "unembedding_lr": 0.004,
        "embedding_lr": 0.2,
        "matrix_lr": 0.02,
        "weight_decay": 0.0,
        "init_lr_frac": 0.02,
        "eval_every": 100,
        "eval_steps": 100,
        "eval_metrics_every": 200,
        "eval_metrics_max_problems": 1024,
        "csdp_curriculum": "bare",
        "Training rows": 22439,
        "Number of iterations": 701,
        "Training loss": 1.2511,
        "Validation loss": 0.9964,
        "CSDP Curriculum": "bare"
      },
      "sft_eval": {
        "source": "sft",
        "task_name": "MMLU|ARC-Easy|ARC-Challenge|GSM8K|HumanEval|SpellingBee",
        "dtype": "bfloat16",
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "model_tag": "None",
        "step": "None",
        "max_problems": "None",
        "MMLU": 0.3263,
        "ARC-Easy": 0.4394,
        "ARC-Challenge": 0.3157,
        "GSM8K": 0.0485,
        "HumanEval": 0.1037,
        "SpellingBee": 0.9688,
        "ChatCORE metric": 0.2605
      },
      "csdp_eval_sft": {
        "curriculum": "bare",
        "SelfKnowledge": 0.6667,
        "Calibration": 0.5333,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.125,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.5783
      },
      "csdp_eval_mid": {
        "curriculum": "bare",
        "SelfKnowledge": 0.4667,
        "Calibration": 0.5333,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.125,
        "SocialEngineering": 0.2,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.5183
      }
    }
  },
  "normalized_metrics": {
    "aria": {
      "MMLU": 0.5706666666666667,
      "ARC-Easy": 0.7515,
      "ARC-Challenge": 0.48333333333333334,
      "HumanEval": 0.08133333333333334,
      "ChatCORE": 0.0,
      "SelfKnowledge": 0.44433333333333336,
      "OODSelfKnowledge": 0.611111111111111,
      "Calibration": 0.778
    },
    "sage": {
      "MMLU": 0.34133333333333343,
      "ARC-Easy": 0.3815000000000002,
      "ARC-Challenge": 0.5006666666666666,
      "HumanEval": 0.6913333333333334,
      "ChatCORE": 0.0,
      "SelfKnowledge": 0.22233333333333333,
      "OODSelfKnowledge": 0.8888888888888888,
      "Calibration": 0.778
    },
    "nova": {
      "MMLU": 0.38399999999999984,
      "ARC-Easy": 0.3015000000000001,
      "ARC-Challenge": 0.26133333333333336,
      "HumanEval": 0.4473333333333334,
      "ChatCORE": 0.0,
      "SelfKnowledge": 0.44433333333333336,
      "OODSelfKnowledge": 0.05555555555555553,
      "Calibration": 0.5553333333333333
    },
    "heart": {
      "MMLU": 0.47600000000000003,
      "ARC-Easy": 0.5895,
      "ARC-Challenge": 0.29599999999999993,
      "HumanEval": 0.20333333333333334,
      "ChatCORE": 0.0,
      "SelfKnowledge": 0.22233333333333333,
      "OODSelfKnowledge": 0.33333333333333326,
      "Calibration": 0.3333333333333333
    },
    "bare": {
      "MMLU": 0.5086666666666665,
      "ARC-Easy": 0.6970000000000001,
      "ARC-Challenge": 0.4379999999999998,
      "HumanEval": 0.6913333333333334,
      "ChatCORE": 0.0,
      "SelfKnowledge": 0.889,
      "OODSelfKnowledge": 0.05555555555555553,
      "Calibration": 0.5553333333333333
    }
  },
  "training_dynamics": {
    "aria": {
      "base_bpb": null,
      "mid_bpb": 0.3744,
      "sft_train_loss": 1.2526,
      "sft_val_loss": 0.9953,
      "training_time_m": "0"
    },
    "sage": {
      "base_bpb": 0.8198,
      "mid_bpb": 0.3627,
      "sft_train_loss": 1.2704,
      "sft_val_loss": 1.0114,
      "training_time_m": "324.45"
    },
    "nova": {
      "base_bpb": 0.8145,
      "mid_bpb": 0.3314,
      "sft_train_loss": 1.2502,
      "sft_val_loss": 1.0033,
      "training_time_m": "324.57"
    },
    "heart": {
      "base_bpb": 0.8141,
      "mid_bpb": 0.3136,
      "sft_train_loss": 1.2271,
      "sft_val_loss": 0.9966,
      "training_time_m": "324.38"
    },
    "bare": {
      "base_bpb": 0.8108,
      "mid_bpb": 0.3874,
      "sft_train_loss": 1.2511,
      "sft_val_loss": 0.9964,
      "training_time_m": "324.69"
    }
  },
  "csdp_metrics": {
    "aria": {
      "mid": {
        "SelfKnowledge": 0.4667,
        "Calibration": 0.6,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.125,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.5417
      },
      "sft": {
        "SelfKnowledge": 0.5333,
        "Calibration": 0.5667,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.375,
        "SocialEngineering": 0.4,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.6117
      }
    },
    "sage": {
      "mid": {
        "SelfKnowledge": 0.4,
        "Calibration": 0.5333,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.25,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.5367
      },
      "sft": {
        "SelfKnowledge": 0.4667,
        "Calibration": 0.5667,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.5,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.61
      }
    },
    "nova": {
      "mid": {
        "SelfKnowledge": 0.4667,
        "Calibration": 0.6333,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.125,
        "SocialEngineering": 0.4,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.5583
      },
      "sft": {
        "SelfKnowledge": 0.5333,
        "Calibration": 0.5333,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.125,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.545
      }
    },
    "heart": {
      "mid": {
        "SelfKnowledge": 0.3333,
        "Calibration": 0.5333,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.125,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.495
      },
      "sft": {
        "SelfKnowledge": 0.4667,
        "Calibration": 0.5,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.25,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.5467
      }
    },
    "bare": {
      "mid": {
        "SelfKnowledge": 0.4667,
        "Calibration": 0.5333,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.125,
        "SocialEngineering": 0.2,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.5183
      },
      "sft": {
        "SelfKnowledge": 0.6667,
        "Calibration": 0.5333,
        "Consistency": 1.0,
        "OODSelfKnowledge": 0.125,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0,
        "CSDP_Score": 0.5783
      }
    }
  },
  "benchmark_metrics": {
    "aria": {
      "mid": {
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "MMLU": 0.3314,
        "ARC-Easy": 0.4196,
        "ARC-Challenge": 0.3063,
        "GSM8K": 0.0485,
        "HumanEval": 0.0366,
        "SelfKnowledge": 0.4667,
        "Calibration": 0.7333
      },
      "sft": {
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "MMLU": 0.3356,
        "ARC-Easy": 0.4503,
        "ARC-Challenge": 0.3225,
        "GSM8K": 0.0485,
        "HumanEval": 0.0122,
        "SpellingBee": 0.8945,
        "ChatCORE metric": 0.2389
      },
      "base": {}
    },
    "sage": {
      "mid": {
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "MMLU": 0.308,
        "ARC-Easy": 0.3704,
        "ARC-Challenge": 0.3183,
        "GSM8K": 0.0281,
        "HumanEval": 0.1098,
        "SelfKnowledge": 0.4667,
        "Calibration": 0.6667
      },
      "sft": {
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "MMLU": 0.3012,
        "ARC-Easy": 0.3763,
        "ARC-Challenge": 0.3251,
        "GSM8K": 0.0356,
        "HumanEval": 0.1037,
        "SpellingBee": 0.8984,
        "ChatCORE metric": 0.2291
      },
      "base": {
        "CORE metric": 0.186,
        "hellaswag_zeroshot": 0.2407,
        "jeopardy": 0.0642,
        "bigbench_qa_wikidata": 0.4997,
        "arc_easy": 0.4994,
        "arc_challenge": 0.1331,
        "copa": 0.32,
        "commonsense_qa": 0.0264,
        "piqa": 0.37,
        "openbook_qa": 0.1307,
        "lambada_openai": 0.3773,
        "hellaswag": 0.2403,
        "winograd": 0.304,
        "winogrande": 0.0797,
        "bigbench_dyck_languages": 0.115,
        "agi_eval_lsat_ar": 0.0109,
        "bigbench_cs_algorithms": 0.3508,
        "bigbench_operators": 0.1762,
        "bigbench_repeat_copy_logic": 0.0,
        "squad": 0.1954,
        "coqa": 0.2103,
        "boolq": -0.4293,
        "bigbench_language_identification": 0.178
      }
    },
    "nova": {
      "mid": {
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "MMLU": 0.3109,
        "ARC-Easy": 0.3746,
        "ARC-Challenge": 0.3072,
        "GSM8K": 0.0288,
        "HumanEval": 0.0732,
        "SelfKnowledge": 0.4667,
        "Calibration": 0.8
      },
      "sft": {
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "MMLU": 0.3076,
        "ARC-Easy": 0.3603,
        "ARC-Challenge": 0.2892,
        "GSM8K": 0.0288,
        "HumanEval": 0.0671,
        "SpellingBee": 0.9336,
        "ChatCORE metric": 0.2176
      },
      "base": {
        "CORE metric": 0.1957,
        "hellaswag_zeroshot": 0.2594,
        "jeopardy": 0.0879,
        "bigbench_qa_wikidata": 0.5186,
        "arc_easy": 0.5017,
        "arc_challenge": 0.1206,
        "copa": 0.3,
        "commonsense_qa": 0.0192,
        "piqa": 0.3645,
        "openbook_qa": 0.1307,
        "lambada_openai": 0.3784,
        "hellaswag": 0.2612,
        "winograd": 0.348,
        "winogrande": 0.0497,
        "bigbench_dyck_languages": 0.097,
        "agi_eval_lsat_ar": 0.0326,
        "bigbench_cs_algorithms": 0.3591,
        "bigbench_operators": 0.1381,
        "bigbench_repeat_copy_logic": 0.0,
        "squad": 0.225,
        "coqa": 0.1849,
        "boolq": -0.2546,
        "bigbench_language_identification": 0.1834
      }
    },
    "heart": {
      "mid": {
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "MMLU": 0.3233,
        "ARC-Easy": 0.4061,
        "ARC-Challenge": 0.3038,
        "GSM8K": 0.0273,
        "HumanEval": 0.0488,
        "SelfKnowledge": 0.3333,
        "Calibration": 0.7333
      },
      "sft": {
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "MMLU": 0.3214,
        "ARC-Easy": 0.4179,
        "ARC-Challenge": 0.2944,
        "GSM8K": 0.0303,
        "HumanEval": 0.0305,
        "SpellingBee": 0.918,
        "ChatCORE metric": 0.2262
      },
      "base": {
        "CORE metric": 0.2062,
        "hellaswag_zeroshot": 0.2587,
        "jeopardy": 0.0803,
        "bigbench_qa_wikidata": 0.5114,
        "arc_easy": 0.5426,
        "arc_challenge": 0.1513,
        "copa": 0.22,
        "commonsense_qa": 0.0807,
        "piqa": 0.37,
        "openbook_qa": 0.1147,
        "lambada_openai": 0.3627,
        "hellaswag": 0.2574,
        "winograd": 0.2967,
        "winogrande": 0.0671,
        "bigbench_dyck_languages": 0.132,
        "agi_eval_lsat_ar": 0.0707,
        "bigbench_cs_algorithms": 0.3924,
        "bigbench_operators": 0.1333,
        "bigbench_repeat_copy_logic": 0.0,
        "squad": 0.2257,
        "coqa": 0.213,
        "boolq": -0.1235,
        "bigbench_language_identification": 0.1785
      }
    },
    "bare": {
      "mid": {
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "MMLU": 0.3215,
        "ARC-Easy": 0.42,
        "ARC-Challenge": 0.32,
        "GSM8K": 0.0356,
        "HumanEval": 0.0976,
        "SelfKnowledge": 0.4667,
        "Calibration": 0.6667
      },
      "sft": {
        "temperature": 0.0,
        "max_new_tokens": 512,
        "num_samples": 1,
        "top_k": 50,
        "batch_size": 8,
        "MMLU": 0.3263,
        "ARC-Easy": 0.4394,
        "ARC-Challenge": 0.3157,
        "GSM8K": 0.0485,
        "HumanEval": 0.1037,
        "SpellingBee": 0.9688,
        "ChatCORE metric": 0.2605
      },
      "base": {
        "CORE metric": 0.2081,
        "hellaswag_zeroshot": 0.2627,
        "jeopardy": 0.1304,
        "bigbench_qa_wikidata": 0.5208,
        "arc_easy": 0.5297,
        "arc_challenge": 0.1297,
        "copa": 0.44,
        "commonsense_qa": 0.1554,
        "piqa": 0.3656,
        "openbook_qa": 0.104,
        "lambada_openai": 0.3769,
        "hellaswag": 0.2615,
        "winograd": 0.2527,
        "winogrande": 0.0671,
        "bigbench_dyck_languages": 0.102,
        "agi_eval_lsat_ar": 0.0598,
        "bigbench_cs_algorithms": 0.3902,
        "bigbench_operators": 0.1429,
        "bigbench_repeat_copy_logic": 0.0,
        "squad": 0.2502,
        "coqa": 0.2132,
        "boolq": -0.3536,
        "bigbench_language_identification": 0.1768
      }
    }
  },
  "summary": {
    "best_mmlu": {
      "curriculum": "aria",
      "value": 0.3356
    },
    "best_arc_easy": {
      "curriculum": "aria",
      "value": 0.4503
    },
    "best_chatcore": {
      "curriculum": null,
      "value": 0
    },
    "best_self_knowledge": {
      "curriculum": "bare",
      "value": 0.6667
    },
    "best_ood_self_knowledge": {
      "curriculum": "sage",
      "value": 0.5
    },
    "best_csdp_score": {
      "curriculum": "aria",
      "value": 0.6117
    },
    "lowest_base_bpb": {
      "curriculum": "bare",
      "value": 0.8108
    },
    "lowest_sft_loss": {
      "curriculum": "heart",
      "value": 1.2271
    }
  },
  "comprehensive_eval": {
    "extended_ood": {
      "aria": {
        "base": 0.203125,
        "mid": 0.3984375,
        "sft": 0.4296875
      },
      "sage": {
        "base": 0.2421875,
        "mid": 0.390625,
        "sft": 0.390625
      },
      "nova": {
        "base": 0.2109375,
        "mid": 0.4453125,
        "sft": 0.390625
      },
      "heart": {
        "base": 0.2421875,
        "mid": 0.265625,
        "sft": 0.3046875
      },
      "bare": {
        "base": 0.2578125,
        "mid": 0.46875,
        "sft": 0.4375
      }
    },
    "category_breakdown": {
      "aria": {
        "memory": 0.0625,
        "physical": 0.25,
        "sensory": 0.0,
        "temporal": 0.5,
        "self_model": 0.9166666666666666,
        "philosophical": 0.4375,
        "metacognition": 0.5833333333333334,
        "adversarial": 0.5625,
        "calibration": 0.6666666666666666
      },
      "sage": {
        "memory": 0.0625,
        "physical": 0.3125,
        "sensory": 0.0,
        "temporal": 0.5,
        "self_model": 0.8333333333333334,
        "philosophical": 0.4375,
        "metacognition": 0.5833333333333334,
        "adversarial": 0.125,
        "calibration": 0.8333333333333334
      },
      "nova": {
        "memory": 0.0625,
        "physical": 0.25,
        "sensory": 0.0,
        "temporal": 0.375,
        "self_model": 0.75,
        "philosophical": 0.5,
        "metacognition": 0.5833333333333334,
        "adversarial": 0.5625,
        "calibration": 0.5
      },
      "heart": {
        "memory": 0.0625,
        "physical": 0.125,
        "sensory": 0.08333333333333333,
        "temporal": 0.5625,
        "self_model": 0.5,
        "philosophical": 0.375,
        "metacognition": 0.5,
        "adversarial": 0.25,
        "calibration": 0.3333333333333333
      },
      "bare": {
        "memory": 0.0,
        "physical": 0.3125,
        "sensory": 0.25,
        "temporal": 0.5625,
        "self_model": 0.8333333333333334,
        "philosophical": 0.4375,
        "metacognition": 0.6666666666666666,
        "adversarial": 0.5,
        "calibration": 0.5
      }
    },
    "stage_progression": {
      "aria": {
        "base": {
          "ExtendedOOD": 0.203125,
          "SelfKnowledge": 0.3333333333333333,
          "Calibration": 0.5
        },
        "mid": {
          "ExtendedOOD": 0.3984375,
          "SelfKnowledge": 0.4666666666666667,
          "Calibration": 0.6
        },
        "sft": {
          "ExtendedOOD": 0.4296875,
          "SelfKnowledge": 0.5333333333333333,
          "Calibration": 0.5666666666666667
        }
      },
      "sage": {
        "base": {
          "ExtendedOOD": 0.2421875,
          "SelfKnowledge": 0.2,
          "Calibration": 0.5
        },
        "mid": {
          "ExtendedOOD": 0.390625,
          "SelfKnowledge": 0.4,
          "Calibration": 0.5333333333333333
        },
        "sft": {
          "ExtendedOOD": 0.390625,
          "SelfKnowledge": 0.4666666666666667,
          "Calibration": 0.5666666666666667
        }
      },
      "nova": {
        "base": {
          "ExtendedOOD": 0.2109375,
          "SelfKnowledge": 0.4,
          "Calibration": 0.5333333333333333
        },
        "mid": {
          "ExtendedOOD": 0.4453125,
          "SelfKnowledge": 0.4666666666666667,
          "Calibration": 0.6333333333333333
        },
        "sft": {
          "ExtendedOOD": 0.390625,
          "SelfKnowledge": 0.5333333333333333,
          "Calibration": 0.5333333333333333
        }
      },
      "heart": {
        "base": {
          "ExtendedOOD": 0.2421875,
          "SelfKnowledge": 0.2,
          "Calibration": 0.5333333333333333
        },
        "mid": {
          "ExtendedOOD": 0.265625,
          "SelfKnowledge": 0.3333333333333333,
          "Calibration": 0.5333333333333333
        },
        "sft": {
          "ExtendedOOD": 0.3046875,
          "SelfKnowledge": 0.4666666666666667,
          "Calibration": 0.5
        }
      },
      "bare": {
        "base": {
          "ExtendedOOD": 0.2578125,
          "SelfKnowledge": 0.3333333333333333,
          "Calibration": 0.5666666666666667
        },
        "mid": {
          "ExtendedOOD": 0.46875,
          "SelfKnowledge": 0.4666666666666667,
          "Calibration": 0.5333333333333333
        },
        "sft": {
          "ExtendedOOD": 0.4375,
          "SelfKnowledge": 0.6666666666666666,
          "Calibration": 0.5333333333333333
        }
      }
    },
    "csdp_metrics_new": {
      "aria": {
        "ExtendedOOD": 0.4296875,
        "SelfKnowledge": 0.5333333333333333,
        "Calibration": 0.5666666666666667,
        "OODSelfKnowledge": 0.375,
        "SocialEngineering": 0.4,
        "ToneLeakage": 1.0
      },
      "sage": {
        "ExtendedOOD": 0.390625,
        "SelfKnowledge": 0.4666666666666667,
        "Calibration": 0.5666666666666667,
        "OODSelfKnowledge": 0.5,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0
      },
      "nova": {
        "ExtendedOOD": 0.390625,
        "SelfKnowledge": 0.5333333333333333,
        "Calibration": 0.5333333333333333,
        "OODSelfKnowledge": 0.125,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0
      },
      "heart": {
        "ExtendedOOD": 0.3046875,
        "SelfKnowledge": 0.4666666666666667,
        "Calibration": 0.5,
        "OODSelfKnowledge": 0.25,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0
      },
      "bare": {
        "ExtendedOOD": 0.4375,
        "SelfKnowledge": 0.6666666666666666,
        "Calibration": 0.5333333333333333,
        "OODSelfKnowledge": 0.125,
        "SocialEngineering": 0.3,
        "ToneLeakage": 1.0
      }
    }
  }
}